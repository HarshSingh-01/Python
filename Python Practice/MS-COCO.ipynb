{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PF-Assgn-33\n",
    "\n",
    "def find_common_characters(msg1,msg2):\n",
    "    out = \"\"\n",
    "    i = 0\n",
    "    while (i <= len(msg1)-1):\n",
    "        if msg1[i] == msg2[i]:\n",
    "            out = out + msg1[i]\n",
    "        i = i+1\n",
    "    return out\n",
    "        \n",
    "#Remove pass and write your logic here\n",
    "\n",
    "#Provide different values for msg1,msg2 and test your program\n",
    "msg1=\"I like Python\"\n",
    "msg2=\"Java is a very popular language\"\n",
    "common_characters=find_common_characters(msg1,msg2)\n",
    "print(common_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import collections\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import os \n",
    "import shutil\n",
    "import time \n",
    "import json\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from IPython.display import display \n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from pickle import dump, load\n",
    "\n",
    "checkpoint_path = \"./train\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder, decoder=decoder, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
    "  # restoring the latest checkpoint in checkpoint_path\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "\n",
    "class Attention(tf.keras.Model):\n",
    "  def __init__(self, units):\n",
    "    super(Attention, self).__init__()\n",
    "    self.W1 = tf.keras.layers.Dense(units)\n",
    "    self.W2 = tf.keras.layers.Dense(units)\n",
    "    self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "  def call(self, features, hidden):\n",
    "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
    "\n",
    "    # hidden shape == (batch_size, hidden_size)\n",
    "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "\n",
    "    # attention_hidden_layer shape == (batch_size, 64, units)\n",
    "    attention_hidden_layer = (tf.nn.tanh(self.W1(features) +\n",
    "                                         self.W2(hidden_with_time_axis)))\n",
    "\n",
    "    # score shape == (batch_size, 64, 1)\n",
    "    # This gives you an unnormalized score for each image feature.\n",
    "    score = self.V(attention_hidden_layer)\n",
    "\n",
    "    # attention_weights shape == (batch_size, 64, 1)\n",
    "    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "    # context_vector shape after sum == (batch_size, hidden_size)\n",
    "    context_vector = attention_weights * features\n",
    "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "    return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim):\n",
    "    super(CNN_Encoder, self).__init__()\n",
    "    # shape after fc == (batch_size, 64, embedding_dim)\n",
    "    self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.fc(x)\n",
    "    x = tf.nn.relu(x)\n",
    "    return x\n",
    " \n",
    "class RNN_Decoder(tf.keras.Model):\n",
    "  def __init__(self, embedding_dim, units, vocab_size):\n",
    "    super(RNN_Decoder, self).__init__()\n",
    "    self.units = units\n",
    "\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(self.units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "    \n",
    "    # self.lstm = tf.keras.layers.LSTM(self.units,\n",
    "    #                                    return_sequences=True,\n",
    "    #                                    return_state=True,\n",
    "    #                                    recurrent_initializer='glorot_uniform')\n",
    "      \n",
    "    # self.forward_layer = tf.keras.layers.LSTM(self.units,\n",
    "    #                                    return_sequences=True,\n",
    "    #                                    return_state=True,\n",
    "    #                                    recurrent_initializer='glorot_uniform')\n",
    "      \n",
    "    # self.backward_layer = tf.keras.layers.LSTM(self.units,\n",
    "    #                                    return_sequences=True,\n",
    "    #                                    return_state=True,\n",
    "    #                                    recurrent_initializer='glorot_uniform',\n",
    "    #                                    go_backwards=True)\n",
    "      \n",
    "    # self.bidirectional = tf.keras.layers.Bidirectional(forward_layer=self.forward_layer, \n",
    "    #                                                     backward_layer=self.backward_layer)\n",
    "\n",
    "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
    "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    self.attention = Attention(self.units)\n",
    "\n",
    "  def call(self, x, features, hidden):\n",
    "    # defining attention as a separate model\n",
    "    context_vector, attention_weights = self.attention(features, hidden)\n",
    "\n",
    "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "    x = self.embedding(x)\n",
    "\n",
    "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "    # passing the concatenated vector to the GRU\n",
    "    output, state = self.gru(x)\n",
    "\n",
    "    # shape == (batch_size, max_length, hidden_size)\n",
    "    x = self.fc1(output)\n",
    "\n",
    "    # x shape == (batch_size * max_length, hidden_size)\n",
    "    x = tf.reshape(x, (-1, x.shape[2]))\n",
    "\n",
    "    # output shape == (batch_size * max_length, vocab)\n",
    "    x = self.fc2(x)\n",
    "\n",
    "    return x, state, attention_weights\n",
    "\n",
    "  def reset_state(self, batch_size):\n",
    "    return tf.zeros((batch_size, self.units))\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "  img = tf.io.read_file(image_path)\n",
    "  img = tf.image.decode_jpeg(img, channels=3)\n",
    "  img = tf.image.resize(img, (299, 299))\n",
    "  img = tf.keras.applications.inception_resnet_v2.preprocess_input(img)\n",
    "  return img, image_path\n",
    "\n",
    "def evaluate(image):\n",
    "    \n",
    "    #attention_features_shape = 64 # To be filled after training on whole dataset\n",
    "    #max_length = 52 # Same as above\n",
    "\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    image_model = tf.keras.applications.InceptionResNetV2(include_top=False, weights='imagenet') #, pooling='max') \n",
    "\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    image_features_extract_model = tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = image_features_extract_model(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    encoder = CNN_Encoder(embedding_dim)\n",
    "    decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    #tokenizer = load(open(\"tokenizer.p\",\"rb\")) #after saving the tokenizer file uncomment that\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
    "        result.append(tokenizer.index_word[predicted_id])\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot\n",
    "\n",
    "# image_url = 'download.jpg'\n",
    "# image_extension = image_url[-4:]\n",
    "# image_path = tf.keras.utils.get_file('image'+ image_extension, origin=image_url)\n",
    "\n",
    "image_path = './download.jpg'\n",
    "result, attention_plot = evaluate(image_path)\n",
    "display(Image.open(image_path))\n",
    "\n",
    "print('Predicted Caption:<start>',' '.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class temp:\n",
    "    a = 5\n",
    "    def __init__(self, b):\n",
    "        self.b = temp.a\n",
    "t = temp(2)\n",
    "t.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
